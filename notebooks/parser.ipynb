{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/data/\"\n",
    "HM_names = [\"H3K27me3\", \"H3K26me3\", \"H3K4me1\", \"H3K4me3\", \"H3K9me3\"]\n",
    "columns_names = [\"geneID\", \"binID\", \"HM1\", \"HM2\", \"HM3\", \"HM4\", \"HM5\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(folder_name):    \n",
    "    # ? test.csv\n",
    "    test_df = pd.read_csv(f\"{data_path}{folder_name}/classification/test.csv\", header=None, names=columns_names,)\n",
    "    # ? train.csv\n",
    "    train_df = pd.read_csv(f\"{data_path}{folder_name}/classification/train.csv\", header=None, names=columns_names,)\n",
    "    # ? valid.csv\n",
    "    valid_df = pd.read_csv(f\"{data_path}{folder_name}/classification/valid.csv\", header=None, names=columns_names,)\n",
    "    return (train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, maxElements):\n",
    "    data = []\n",
    "    labels = []\n",
    "    genes_inserted = {}\n",
    "\n",
    "    range_ = range(int(len(df) / 100))\n",
    "    if maxElements:\n",
    "        range_ = range_[:maxElements]\n",
    "    #bar = Bar(\"\\t- creating datasets\", max=len(range_))\n",
    "    for i in range_:  # todo: remove this limit to consider the full dataset\n",
    "        geneID = df.iloc[i * 100][\"geneID\"]\n",
    "        if genes_inserted.get(geneID) is None:\n",
    "            genes_inserted[geneID] = True\n",
    "\n",
    "            df_gene = df.loc[df[\"geneID\"] == geneID]\n",
    "\n",
    "            labels.append(df_gene.iloc[0][\"label\"])  # ? saving the label once and for all the gene\n",
    "            # ? extract the whole list of values for the different HMs as a matrix\n",
    "            # ! I had to put [:100] to limit the errors on the input data (e.i. some genes where duplicates)\n",
    "            gene_data = [\n",
    "                list(df_gene[\"HM1\"])[:100],\n",
    "                list(df_gene[\"HM2\"])[:100],\n",
    "                list(df_gene[\"HM3\"])[:100],\n",
    "                list(df_gene[\"HM4\"])[:100],\n",
    "                list(df_gene[\"HM5\"])[:100],\n",
    "            ]\n",
    "            data.append(preprocessing.normalize(gene_data))\n",
    "        #bar.next()\n",
    "    #bar.finish()\n",
    "    return (data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(data, labels):\n",
    "    #bar = Bar(\"\\t- converting to numpy format\", max=len(data))\n",
    "    numpy_data = np.array([])\n",
    "    for gene in data:\n",
    "        numpy_gene = np.array([])\n",
    "        for bins in gene:\n",
    "            numpy_gene = np.append(numpy_gene, np.array(bins))\n",
    "        numpy_data = np.append(numpy_data, numpy_gene)\n",
    "        #bar.next()\n",
    "    #bar.finish()\n",
    "    numpy_data = numpy_data.reshape(len(data), 5, 100, 1)\n",
    "    numpy_labels = np.array(labels)\n",
    "    return numpy_data, numpy_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function normalize the datasets using the mean and the standard deviation of the training set.\n",
    "    Each normalization is computed at cell-type level, not at global level.\n",
    "'''\n",
    "def normalize(train_data,valid_data,test_data):\n",
    "\n",
    "    mean = np.mean(train_data)\n",
    "    train_data-=mean\n",
    "    valid_data-=mean\n",
    "    test_data-=mean\n",
    "    \n",
    "    std = np.std(train_data)\n",
    "    train_data/=std\n",
    "    valid_data/=std\n",
    "    test_data/=std    \n",
    "    \n",
    "    return (train_data,valid_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(maxFolders=None, maxElements=None):\n",
    "\n",
    "    full_train_data = np.array([])\n",
    "    full_valid_data = np.array([])\n",
    "    full_test_data = np.array([])\n",
    "    full_train_labels = np.array([])\n",
    "    full_valid_labels = np.array([])\n",
    "    full_test_labels = np.array([])\n",
    "\n",
    "    dirs = os.listdir(data_path)\n",
    "    if maxFolders:\n",
    "        dirs = dirs[:maxFolders]    \n",
    "    for folder in dirs:\n",
    "        folder_files =  os.listdir(data_path+folder)\n",
    "        if len(folder_files)==1:\n",
    "            (train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels) = load_folder(\n",
    "                folder, maxElements\n",
    "            )\n",
    "        else:\n",
    "            (train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels) = load_clean_data(folder)            \n",
    "            print(f\"{folder} loaded\")\n",
    "        \n",
    "        # append to the full np array\n",
    "        \n",
    "        # train\n",
    "        full_train_data = np.append(full_train_data, train_data)\n",
    "        full_train_labels = np.append(full_train_labels, train_labels)\n",
    "        # valid\n",
    "        full_valid_data = np.append(full_valid_data, valid_data)\n",
    "        full_valid_labels = np.append(full_valid_labels, valid_labels)\n",
    "        # test\n",
    "        full_test_data = np.append(full_test_data, test_data)\n",
    "        full_test_labels = np.append(full_test_labels, test_labels)\n",
    "\n",
    "    # ? reshaping\n",
    "    full_train_data = full_train_data.reshape(len(full_train_labels), 5, 100, 1)\n",
    "    full_valid_data = full_valid_data.reshape(len(full_valid_labels), 5, 100, 1)\n",
    "    full_test_data = full_test_data.reshape(len(full_test_labels), 5, 100, 1)\n",
    "\n",
    "    return (\n",
    "        (full_train_data, full_train_labels),\n",
    "        (full_valid_data, full_valid_labels),\n",
    "        (full_test_data, full_test_labels),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load the data contained in one folder\n",
    "'''\n",
    "def load_folder(folder_name, maxElements=None):\n",
    "    print(folder_name)\n",
    "    # loading the datasets\n",
    "    print(\"\\tloading\")\n",
    "    train_df, valid_df, test_df = load_datasets(folder_name)\n",
    "\n",
    "    # create the datasets with the correct format\n",
    "    print(\"\\tcreating datasets\")\n",
    "    train_data, train_labels = create_dataset(train_df, maxElements)\n",
    "    valid_data, valid_labels = create_dataset(valid_df, maxElements)\n",
    "    test_data, test_labels = create_dataset(test_df, maxElements)\n",
    "\n",
    "    # numpy arrays\n",
    "    print(\"\\tto numpy\")\n",
    "    train_data, train_labels = to_numpy(train_data, train_labels)\n",
    "    valid_data, valid_labels = to_numpy(valid_data, valid_labels)\n",
    "    test_data, test_labels = to_numpy(test_data, test_labels)\n",
    "    \n",
    "    print(\"\\tnormalize\")\n",
    "    train_data,valid_data,test_data = normalize(train_data,valid_data,test_data)\n",
    "    \n",
    "    # saving data to file\n",
    "    print(\"\\tsaving data\")\n",
    "    save_data(folder_name, train_data, train_labels, valid_data, valid_labels, test_data, test_labels)\n",
    "\n",
    "    # return tris of tuples (data,labels)\n",
    "    return ((train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    save the data in the correct format in order to speed up future loading time\n",
    "'''\n",
    "def save_data(folder_name, train_data, train_labels, valid_data, valid_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(len(train_data),5,100)\n",
    "    valid_data = valid_data.reshape(len(valid_data),5,100)\n",
    "    test_data = test_data.reshape(len(test_data),5,100)\n",
    "    # data\n",
    "    np.save(f\"{data_path}{folder_name}/train_data.npy\", train_data)\n",
    "    np.save(f\"{data_path}{folder_name}/valid_data.npy\", valid_data)\n",
    "    np.save(f\"{data_path}{folder_name}/test_data.npy\", test_data)\n",
    "    # labels\n",
    "    np.save(f\"{data_path}{folder_name}/train_labels.npy\", train_labels)\n",
    "    np.save(f\"{data_path}{folder_name}/valid_labels.npy\", valid_labels)\n",
    "    np.save(f\"{data_path}{folder_name}/test_labels.npy\", test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    loads to memory the already cleaned data\n",
    "'''\n",
    "def load_clean_data(folder_name):\n",
    "    # data\n",
    "    train_data = np.load(f\"{data_path}{folder_name}/train_data.npy\")\n",
    "    valid_data = np.load(f\"{data_path}{folder_name}/valid_data.npy\")\n",
    "    test_data = np.load(f\"{data_path}{folder_name}/test_data.npy\")\n",
    "    # labels\n",
    "    train_labels = np.load(f\"{data_path}{folder_name}/train_labels.npy\")\n",
    "    valid_labels = np.load(f\"{data_path}{folder_name}/valid_labels.npy\")\n",
    "    test_labels = np.load(f\"{data_path}{folder_name}/test_labels.npy\")\n",
    "    # return complete tuples\n",
    "    return ((train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E003 loaded\n",
      "E003 normalized\n",
      "E003 re-saved\n",
      "E004 loaded\n",
      "E004 normalized\n",
      "E004 re-saved\n",
      "E005 loaded\n",
      "E005 normalized\n",
      "E005 re-saved\n",
      "E006 loaded\n",
      "E006 normalized\n",
      "E006 re-saved\n",
      "E007 loaded\n",
      "E007 normalized\n",
      "E007 re-saved\n",
      "E011 loaded\n",
      "E011 normalized\n",
      "E011 re-saved\n",
      "E012 loaded\n",
      "E012 normalized\n",
      "E012 re-saved\n",
      "E013 loaded\n",
      "E013 normalized\n",
      "E013 re-saved\n",
      "E016 loaded\n",
      "E016 normalized\n",
      "E016 re-saved\n",
      "E024 loaded\n",
      "E024 normalized\n",
      "E024 re-saved\n",
      "E027 loaded\n",
      "E027 normalized\n",
      "E027 re-saved\n",
      "E028 loaded\n",
      "E028 normalized\n",
      "E028 re-saved\n",
      "E037 loaded\n",
      "E037 normalized\n",
      "E037 re-saved\n",
      "E038 loaded\n",
      "E038 normalized\n",
      "E038 re-saved\n",
      "E047 loaded\n",
      "E047 normalized\n",
      "E047 re-saved\n",
      "E050 loaded\n",
      "E050 normalized\n",
      "E050 re-saved\n",
      "E053 loaded\n",
      "E053 normalized\n",
      "E053 re-saved\n",
      "E054 loaded\n",
      "E054 normalized\n",
      "E054 re-saved\n",
      "E055 loaded\n",
      "E055 normalized\n",
      "E055 re-saved\n",
      "E056 loaded\n",
      "E056 normalized\n",
      "E056 re-saved\n",
      "E057 loaded\n",
      "E057 normalized\n",
      "E057 re-saved\n",
      "E058 loaded\n",
      "E058 normalized\n",
      "E058 re-saved\n",
      "E059 loaded\n",
      "E059 normalized\n",
      "E059 re-saved\n",
      "E061 loaded\n",
      "E061 normalized\n",
      "E061 re-saved\n",
      "E062 loaded\n",
      "E062 normalized\n",
      "E062 re-saved\n",
      "E065 loaded\n",
      "E065 normalized\n",
      "E065 re-saved\n",
      "E066 loaded\n",
      "E066 normalized\n",
      "E066 re-saved\n",
      "E070 loaded\n",
      "E070 normalized\n",
      "E070 re-saved\n",
      "E071 loaded\n",
      "E071 normalized\n",
      "E071 re-saved\n",
      "E079 loaded\n",
      "E079 normalized\n",
      "E079 re-saved\n",
      "E082 loaded\n",
      "E082 normalized\n",
      "E082 re-saved\n",
      "E084 loaded\n",
      "E084 normalized\n",
      "E084 re-saved\n",
      "E085 loaded\n",
      "E085 normalized\n",
      "E085 re-saved\n",
      "E087 loaded\n",
      "E087 normalized\n",
      "E087 re-saved\n",
      "E094 loaded\n",
      "E094 normalized\n",
      "E094 re-saved\n",
      "E095 loaded\n",
      "E095 normalized\n",
      "E095 re-saved\n",
      "E096 loaded\n",
      "E096 normalized\n",
      "E096 re-saved\n",
      "E097 loaded\n",
      "E097 normalized\n",
      "E097 re-saved\n",
      "E098 loaded\n",
      "E098 normalized\n",
      "E098 re-saved\n",
      "E100 loaded\n",
      "E100 normalized\n",
      "E100 re-saved\n",
      "E104 loaded\n",
      "E104 normalized\n",
      "E104 re-saved\n",
      "E105 loaded\n",
      "E105 normalized\n",
      "E105 re-saved\n",
      "E106 loaded\n",
      "E106 normalized\n",
      "E106 re-saved\n",
      "E109 loaded\n",
      "E109 normalized\n",
      "E109 re-saved\n",
      "E112 loaded\n",
      "E112 normalized\n",
      "E112 re-saved\n",
      "E113 loaded\n",
      "E113 normalized\n",
      "E113 re-saved\n",
      "E114 loaded\n",
      "E114 normalized\n",
      "E114 re-saved\n",
      "E116 loaded\n",
      "E116 normalized\n",
      "E116 re-saved\n",
      "E117 loaded\n",
      "E117 normalized\n",
      "E117 re-saved\n",
      "E118 loaded\n",
      "E118 normalized\n",
      "E118 re-saved\n",
      "E119 loaded\n",
      "E119 normalized\n",
      "E119 re-saved\n",
      "E120 loaded\n",
      "E120 normalized\n",
      "E120 re-saved\n",
      "E122 loaded\n",
      "E122 normalized\n",
      "E122 re-saved\n",
      "E123 loaded\n",
      "E123 normalized\n",
      "E123 re-saved\n",
      "E127 loaded\n",
      "E127 normalized\n",
      "E127 re-saved\n",
      "E128 loaded\n",
      "E128 normalized\n",
      "E128 re-saved\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels) = load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
