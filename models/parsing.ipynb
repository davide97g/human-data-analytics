{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../dataset/data/\"\n",
    "HM_names = [\"H3K27me3\", \"H3K26me3\", \"H3K4me1\", \"H3K4me3\", \"H3K9me3\"]\n",
    "columns_names = [\"geneID\", \"binID\", \"HM1\", \"HM2\", \"HM3\", \"HM4\", \"HM5\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(folder_name):    \n",
    "    # ? test.csv\n",
    "    test_df = pd.read_csv(f\"{data_path}{folder_name}/classification/test.csv\", header=None, names=columns_names,)\n",
    "    # ? train.csv\n",
    "    train_df = pd.read_csv(f\"{data_path}{folder_name}/classification/train.csv\", header=None, names=columns_names,)\n",
    "    # ? valid.csv\n",
    "    valid_df = pd.read_csv(f\"{data_path}{folder_name}/classification/valid.csv\", header=None, names=columns_names,)\n",
    "    return (train_df, valid_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, maxElements):\n",
    "    data = []\n",
    "    labels = []\n",
    "    genes_inserted = {}\n",
    "\n",
    "    range_ = range(int(len(df) / 100))\n",
    "    if maxElements:\n",
    "        range_ = range_[:maxElements]\n",
    "    #bar = Bar(\"\\t- creating datasets\", max=len(range_))\n",
    "    for i in range_:  # todo: remove this limit to consider the full dataset\n",
    "        geneID = df.iloc[i * 100][\"geneID\"]\n",
    "        if genes_inserted.get(geneID) is None:\n",
    "            genes_inserted[geneID] = True\n",
    "\n",
    "            df_gene = df.loc[df[\"geneID\"] == geneID]\n",
    "\n",
    "            labels.append(df_gene.iloc[0][\"label\"])  # ? saving the label once and for all the gene\n",
    "            # ? extract the whole list of values for the different HMs as a matrix\n",
    "            # ! I had to put [:100] to limit the errors on the input data (e.i. some genes where duplicates)\n",
    "            gene_data = [\n",
    "                list(df_gene[\"HM1\"])[:100],\n",
    "                list(df_gene[\"HM2\"])[:100],\n",
    "                list(df_gene[\"HM3\"])[:100],\n",
    "                list(df_gene[\"HM4\"])[:100],\n",
    "                list(df_gene[\"HM5\"])[:100],\n",
    "            ]\n",
    "            data.append(preprocessing.normalize(gene_data))\n",
    "        #bar.next()\n",
    "    #bar.finish()\n",
    "    return (data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(data, labels):\n",
    "    #bar = Bar(\"\\t- converting to numpy format\", max=len(data))\n",
    "    numpy_data = np.array([])\n",
    "    for gene in data:\n",
    "        numpy_gene = np.array([])\n",
    "        for bins in gene:\n",
    "            numpy_gene = np.append(numpy_gene, np.array(bins))\n",
    "        numpy_data = np.append(numpy_data, numpy_gene)\n",
    "        #bar.next()\n",
    "    #bar.finish()\n",
    "    numpy_data = numpy_data.reshape(len(data), 5, 100, 1)\n",
    "    numpy_labels = np.array(labels)\n",
    "    return numpy_data, numpy_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(maxFolders=None, maxElements=None):\n",
    "\n",
    "    full_train_data = np.array([])\n",
    "    full_valid_data = np.array([])\n",
    "    full_test_data = np.array([])\n",
    "    full_train_labels = np.array([])\n",
    "    full_valid_labels = np.array([])\n",
    "    full_test_labels = np.array([])\n",
    "\n",
    "    dirs = os.listdir(data_path)\n",
    "    if maxFolders:\n",
    "        dirs = dirs[:maxFolders]    \n",
    "    for folder in dirs:\n",
    "        folder_files =  os.listdir(data_path+folder)\n",
    "        if len(folder_files)==1:\n",
    "            (train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels) = load_folder(\n",
    "                folder, maxElements\n",
    "            )\n",
    "        else:\n",
    "            (train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels) = load_clean_data(folder)\n",
    "            print(f\"{folder} loaded\")\n",
    "        \n",
    "        # append to the full np array\n",
    "        \n",
    "        # train\n",
    "        full_train_data = np.append(full_train_data, train_data)\n",
    "        full_train_labels = np.append(full_train_labels, train_labels)\n",
    "        # valid\n",
    "        full_valid_data = np.append(full_valid_data, valid_data)\n",
    "        full_valid_labels = np.append(full_valid_labels, valid_labels)\n",
    "        # test\n",
    "        full_test_data = np.append(full_test_data, test_data)\n",
    "        full_test_labels = np.append(full_test_labels, test_labels)\n",
    "\n",
    "    # ? reshaping\n",
    "    full_train_data = full_train_data.reshape(len(full_train_labels), 5, 100, 1)\n",
    "    full_valid_data = full_valid_data.reshape(len(full_valid_labels), 5, 100, 1)\n",
    "    full_test_data = full_test_data.reshape(len(full_test_labels), 5, 100, 1)\n",
    "\n",
    "    return (\n",
    "        (full_train_data, full_train_labels),\n",
    "        (full_valid_data, full_valid_labels),\n",
    "        (full_test_data, full_test_labels),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folder(folder_name, maxElements=None):\n",
    "    print(folder_name)\n",
    "    # loading the datasets\n",
    "    print(\"\\tloading\")\n",
    "    train_df, valid_df, test_df = load_datasets(folder_name)\n",
    "\n",
    "    # create the datasets with the correct format\n",
    "    print(\"\\tcreating datasets\")\n",
    "    train_data, train_labels = create_dataset(train_df, maxElements)\n",
    "    valid_data, valid_labels = create_dataset(valid_df, maxElements)\n",
    "    test_data, test_labels = create_dataset(test_df, maxElements)\n",
    "\n",
    "    # numpy arrays\n",
    "    print(\"\\tto numpy\")\n",
    "    train_data, train_labels = to_numpy(train_data, train_labels)\n",
    "    valid_data, valid_labels = to_numpy(valid_data, valid_labels)\n",
    "    test_data, test_labels = to_numpy(test_data, test_labels)\n",
    "    \n",
    "    # saving data to file\n",
    "    print(\"\\tsaving data\")\n",
    "    save_data(folder_name, train_data, train_labels, valid_data, valid_labels, test_data, test_labels)\n",
    "\n",
    "    # return tris of tuples (data,labels)\n",
    "    return ((train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    save the data in the correct format in order to speed up future loading time\n",
    "'''\n",
    "def save_data(folder_name, train_data, train_labels, valid_data, valid_labels, test_data, test_labels):\n",
    "    train_data = train_data.reshape(len(train_data),5,100)\n",
    "    valid_data = valid_data.reshape(len(valid_data),5,100)\n",
    "    test_data = test_data.reshape(len(test_data),5,100)\n",
    "    # data\n",
    "    np.save(f\"{data_path}{folder_name}/train_data.npy\", train_data)\n",
    "    np.save(f\"{data_path}{folder_name}/valid_data.npy\", valid_data)\n",
    "    np.save(f\"{data_path}{folder_name}/test_data.npy\", test_data)\n",
    "    # labels\n",
    "    np.save(f\"{data_path}{folder_name}/train_labels.npy\", train_labels)\n",
    "    np.save(f\"{data_path}{folder_name}/valid_labels.npy\", valid_labels)\n",
    "    np.save(f\"{data_path}{folder_name}/test_labels.npy\", test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(folder_name):\n",
    "    # data\n",
    "    train_data = np.load(f\"{data_path}{folder_name}/train_data.npy\")\n",
    "    valid_data = np.load(f\"{data_path}{folder_name}/valid_data.npy\")\n",
    "    test_data = np.load(f\"{data_path}{folder_name}/test_data.npy\")\n",
    "    # labels\n",
    "    train_labels = np.load(f\"{data_path}{folder_name}/train_labels.npy\")\n",
    "    valid_labels = np.load(f\"{data_path}{folder_name}/valid_labels.npy\")\n",
    "    test_labels = np.load(f\"{data_path}{folder_name}/test_labels.npy\")\n",
    "    # return complete tuples\n",
    "    return ((train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E003 loaded\n",
      "E004 loaded\n",
      "E005 loaded\n",
      "E006 loaded\n",
      "E007 loaded\n",
      "E011 loaded\n",
      "E012 loaded\n",
      "E013 loaded\n",
      "E016 loaded\n",
      "E024 loaded\n",
      "E027 loaded\n",
      "E028 loaded\n",
      "E037 loaded\n",
      "E038 loaded\n",
      "E047 loaded\n",
      "E050 loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d65bd9f8a2fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-dde6a912b2cf>\u001b[0m in \u001b[0;36mload\u001b[1;34m(maxFolders, maxElements)\u001b[0m\n\u001b[0;32m     18\u001b[0m             )\n\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{folder} loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-2e0481bd7cd4>\u001b[0m in \u001b[0;36mload_clean_data\u001b[1;34m(folder_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_clean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{data_path}{folder_name}/train_data.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{data_path}{folder_name}/valid_data.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{data_path}{folder_name}/test_data.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[0;32m    440\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (valid_data, valid_labels), (test_data, test_labels) = load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
